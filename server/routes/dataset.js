// Dataset routes for CSV data management
import express from 'express';
import { query } from '../config/database.js';
import { uploadParquetToS3, downloadParquetFromS3, deleteProjectFromS3 } from '../config/s3.js';
import parquet from 'parquetjs';
import { readFile, unlink, writeFile } from 'fs/promises';
import { join } from 'path';
import { tmpdir } from 'os';
import { randomUUID } from 'crypto';

const router = express.Router();

// Save CSV data to database
router.post('/save', async (req, res) => {
  try {
    console.log('Dataset save request received');
    console.log('Session user:', req.session.user);
    
    // Check if user is authenticated
    if (!req.session.user || !req.session.user.sub) {
      console.log('Authentication check failed');
      return res.status(401).json({ error: 'Authentication required' });
    }

    const { projectName, csvData } = req.body;
    const cognitoSub = req.session.user.sub;

    console.log('Request data:', { projectName, hasCsvData: !!csvData, cognitoSub });

    if (!projectName || !csvData) {
      return res.status(400).json({ error: 'Project name and CSV data are required' });
    }

    // Verify user exists in database (required for foreign key)
    const userCheck = await query(
      'SELECT cognito_sub FROM users WHERE cognito_sub = $1',
      [cognitoSub]
    );

    if (userCheck.rows.length === 0) {
      console.log('User not found in database, inserting...');
      // User might not be in database yet, insert them
      const userEmail = req.session.user.email || 'unknown@example.com';
      await query(
        'INSERT INTO users (cognito_sub, email) VALUES ($1, $2) ON CONFLICT (cognito_sub) DO NOTHING',
        [cognitoSub, userEmail]
      );
    }

    // Insert into dataset table
    // ds_group_id and commit_id will be generated by the database
    console.log('Inserting into dataset table...');
    const datasetResult = await query(
      `INSERT INTO dataset (cognito_sub) 
       VALUES ($1) 
       RETURNING ds_group_id, commit_id`,
      [cognitoSub]
    );

    if (datasetResult.rows.length === 0) {
      console.error('Failed to create dataset entry - no rows returned');
      return res.status(500).json({ error: 'Failed to create dataset entry' });
    }

    const { ds_group_id, commit_id } = datasetResult.rows[0];
    console.log('Dataset created:', { ds_group_id, commit_id });

    // Convert CSV data to Parquet
    console.log('Converting CSV to Parquet...');
    let parquetBuffer;
    try {
      parquetBuffer = await convertCsvToParquet(csvData);
      console.log(`✓ Parquet file created (${parquetBuffer.length} bytes)`);
    } catch (parquetError) {
      console.error('Error converting CSV to Parquet:', parquetError);
      // Continue with database operations even if Parquet conversion fails
      // but log the error
    }

    // Upload Parquet file to S3
    let s3Key = null;
    if (parquetBuffer) {
      console.log('Attempting S3 upload...');
      console.log('  Parquet buffer size:', parquetBuffer.length, 'bytes');
      try {
        s3Key = await uploadParquetToS3(parquetBuffer, cognitoSub, ds_group_id, commit_id);
        console.log(`✓ File uploaded to S3: ${s3Key}`);
      } catch (s3Error) {
        console.error('❌ Error uploading to S3:', s3Error);
        console.error('  Error details:', {
          name: s3Error.name,
          message: s3Error.message,
          code: s3Error.code,
        });
        // Continue with database operations even if S3 upload fails
        // but log the error
      }
    } else {
      console.warn('⚠️  Skipping S3 upload - Parquet buffer is empty or conversion failed');
    }

    // Insert into names table
    console.log('Inserting into names table...');
    try {
      await query(
        `INSERT INTO names (name, ds_group_id, root_commit_id, cognito_sub) 
         VALUES ($1, $2, $3, $4) 
         ON CONFLICT (ds_group_id, root_commit_id) 
         DO UPDATE SET name = EXCLUDED.name`,
        [projectName, ds_group_id, commit_id, cognitoSub]
      );
      console.log('Names entry created successfully');
    } catch (error) {
      // Check if it's a unique constraint violation on (cognito_sub, name)
      if (error.code === '23505' && error.constraint === 'names_cognito_sub_name_unique') {
        console.error('Duplicate project name detected');
        return res.status(409).json({ 
          error: 'Project name already exists', 
          details: `A project with the name "${projectName}" already exists. Please choose a different name.`,
          code: 'DUPLICATE_PROJECT_NAME'
        });
      }
      // Re-throw other errors
      throw error;
    }

    // Store the dataset info in session for reference
    req.session.currentDataset = {
      ds_group_id,
      commit_id,
      projectName,
      s3Key
    };

    res.json({ 
      success: true, 
      ds_group_id, 
      commit_id,
      s3Key,
      message: 'CSV data saved successfully' 
    });
  } catch (error) {
    console.error('Error saving CSV data:', error);
    console.error('Error stack:', error.stack);
    res.status(500).json({ error: 'Failed to save CSV data', details: error.message });
  }
});

// Get user's datasets
router.get('/list', async (req, res) => {
  try {
    if (!req.session.user || !req.session.user.sub) {
      return res.status(401).json({ error: 'Authentication required' });
    }

    const cognitoSub = req.session.user.sub;

    // Get all datasets for this user with their names
    const result = await query(
      `SELECT DISTINCT d.ds_group_id, d.commit_id, n.name, d.cognito_sub
       FROM dataset d
       LEFT JOIN names n ON d.ds_group_id = n.ds_group_id AND d.commit_id = n.root_commit_id
       WHERE d.cognito_sub = $1
       ORDER BY d.commit_id DESC`,
      [cognitoSub]
    );

    res.json({ success: true, datasets: result.rows });
  } catch (error) {
    console.error('Error fetching datasets:', error);
    res.status(500).json({ error: 'Failed to fetch datasets', details: error.message });
  }
});

// Get list of project names for the current user
router.get('/projects', async (req, res) => {
  try {
    if (!req.session.user || !req.session.user.sub) {
      return res.status(401).json({ error: 'Authentication required' });
    }

    const cognitoSub = req.session.user.sub;

    // Get distinct project names for this user (root datasets only)
    const result = await query(
      `SELECT DISTINCT n.name, n.ds_group_id, n.root_commit_id
       FROM names n
       INNER JOIN dataset d ON n.ds_group_id = d.ds_group_id AND n.root_commit_id = d.commit_id
       WHERE d.cognito_sub = $1
         AND d.parent_ds_group_id IS NULL
         AND d.parent_commit_id IS NULL
       ORDER BY n.name`,
      [cognitoSub]
    );

    res.json({ success: true, projects: result.rows });
  } catch (error) {
    console.error('Error fetching projects:', error);
    res.status(500).json({ error: 'Failed to fetch projects', details: error.message });
  }
});

// Get root dataset for a project (where parent_ds_group_id and parent_commit_id are null)
router.get('/project/:dsGroupId/root', async (req, res) => {
  try {
    if (!req.session.user || !req.session.user.sub) {
      return res.status(401).json({ error: 'Authentication required' });
    }

    const cognitoSub = req.session.user.sub;
    const { dsGroupId } = req.params;

    // Find the root dataset (where parent_ds_group_id and parent_commit_id are null)
    const result = await query(
      `SELECT ds_group_id, commit_id, cognito_sub
       FROM dataset
       WHERE ds_group_id = $1
         AND cognito_sub = $2
         AND parent_ds_group_id IS NULL
         AND parent_commit_id IS NULL
       LIMIT 1`,
      [dsGroupId, cognitoSub]
    );

    if (result.rows.length === 0) {
      return res.status(404).json({ error: 'Root dataset not found for this project' });
    }

    const { commit_id } = result.rows[0];

    // Download Parquet file from S3
    console.log(`Downloading Parquet file for project ${dsGroupId}, commit ${commit_id}...`);
    try {
      const parquetBuffer = await downloadParquetFromS3(cognitoSub, dsGroupId, commit_id);

      // Convert Parquet to CSV format
      console.log('Converting Parquet to CSV...');
      const csvData = await convertParquetToCsv(parquetBuffer);

      res.json({ 
        success: true, 
        csvData: {
          headers: csvData.headers,
          rows: csvData.rows
        }
      });
    } catch (s3Error) {
      // Handle S3-specific errors
      if (s3Error.name === 'NoSuchKey' || s3Error.Code === 'NoSuchKey') {
        console.error('S3 file not found for this project. The file may not have been uploaded successfully.');
        return res.status(404).json({ 
          error: 'Dataset file not found', 
          details: 'The data file for this project does not exist in storage. This may happen if the file upload failed or was deleted. Please try uploading the project again.',
          code: 'FILE_NOT_FOUND'
        });
      }
      // Re-throw other S3 errors
      throw s3Error;
    }
  } catch (error) {
    console.error('Error fetching root dataset:', error);
    const statusCode = error.name === 'NoSuchKey' || error.Code === 'NoSuchKey' ? 404 : 500;
    res.status(statusCode).json({ 
      error: 'Failed to fetch root dataset', 
      details: error.message,
      code: error.name || error.Code
    });
  }
});

// Delete a project (all commits for a ds_group_id)
router.delete('/project/:dsGroupId', async (req, res) => {
  try {
    if (!req.session.user || !req.session.user.sub) {
      return res.status(401).json({ error: 'Authentication required' });
    }

    const cognitoSub = req.session.user.sub;
    const { dsGroupId } = req.params;

    // Verify the project belongs to this user
    const projectCheck = await query(
      `SELECT ds_group_id FROM dataset 
       WHERE ds_group_id = $1 AND cognito_sub = $2 
       LIMIT 1`,
      [dsGroupId, cognitoSub]
    );

    if (projectCheck.rows.length === 0) {
      return res.status(404).json({ error: 'Project not found or access denied' });
    }

    // Delete all S3 files for this project
    console.log(`Deleting S3 files for project ${dsGroupId}...`);
    let s3DeletedCount = 0;
    try {
      s3DeletedCount = await deleteProjectFromS3(cognitoSub, dsGroupId);
    } catch (s3Error) {
      console.error('Error deleting S3 files:', s3Error);
      // Continue with database deletion even if S3 deletion fails
    }

    // Delete from names table
    console.log('Deleting from names table...');
    await query(
      `DELETE FROM names WHERE ds_group_id = $1`,
      [dsGroupId]
    );

    // Delete from dataset table (this will cascade delete related records)
    console.log('Deleting from dataset table...');
    await query(
      `DELETE FROM dataset WHERE ds_group_id = $1 AND cognito_sub = $2`,
      [dsGroupId, cognitoSub]
    );

    console.log(`✓ Project ${dsGroupId} deleted successfully`);
    res.json({ 
      success: true, 
      message: 'Project deleted successfully',
      s3FilesDeleted: s3DeletedCount
    });
  } catch (error) {
    console.error('Error deleting project:', error);
    res.status(500).json({ error: 'Failed to delete project', details: error.message });
  }
});

/**
 * Convert Parquet file to CSV format
 * @param {Buffer} parquetBuffer - Parquet file as a Buffer
 * @returns {Promise<Object>} - Object with { headers: string[], rows: string[][] }
 */
async function convertParquetToCsv(parquetBuffer) {
  // Write buffer to temporary file
  const tempFilePath = join(tmpdir(), `parquet-${randomUUID()}.parquet`);
  
  try {
    await writeFile(tempFilePath, parquetBuffer);
    
    // Read Parquet file
    const reader = await parquet.ParquetReader.openFile(tempFilePath);
    const cursor = reader.getCursor();
    
    // Read first row to get headers
    const firstRecord = await cursor.next();
    if (!firstRecord) {
      throw new Error('Parquet file is empty');
    }
    
    // Get headers from first record keys
    const headers = Object.keys(firstRecord);
    const headerCount = headers.length;
    
    // Pre-allocate array for better performance (estimate size if possible)
    const rows = [];
    // Add first row - optimized conversion
    const firstRow = new Array(headerCount);
    for (let i = 0; i < headerCount; i++) {
      const value = firstRecord[headers[i]];
      firstRow[i] = value !== null && value !== undefined ? String(value) : '';
    }
    rows.push(firstRow);
    
    // Read remaining rows - optimized: batch processing and pre-allocated arrays
    const BATCH_SIZE = 1000;
    let batch = [];
    let record;
    
    while ((record = await cursor.next())) {
      const row = new Array(headerCount);
      for (let i = 0; i < headerCount; i++) {
        const value = record[headers[i]];
        row[i] = value !== null && value !== undefined ? String(value) : '';
      }
      batch.push(row);
      
      // Process in batches to reduce memory churn
      if (batch.length >= BATCH_SIZE) {
        rows.push(...batch);
        batch = [];
      }
    }
    
    // Add remaining rows
    if (batch.length > 0) {
      rows.push(...batch);
    }
    
    await reader.close();
    
    // Clean up temporary file
    await unlink(tempFilePath).catch(err => {
      console.warn('Failed to delete temporary file:', tempFilePath, err);
    });
    
    return { headers, rows };
  } catch (error) {
    // Clean up temporary file on error
    await unlink(tempFilePath).catch(() => {
      // Ignore errors during cleanup
    });
    throw error;
  }
}

/**
 * Convert CSV data to Parquet format
 * @param {Object} csvData - Object with { headers: string[], rows: string[][] }
 * @returns {Promise<Buffer>} - Parquet file as a Buffer
 */
async function convertCsvToParquet(csvData) {
  const { headers, rows } = csvData;

  if (!headers || !rows || headers.length === 0) {
    throw new Error('Invalid CSV data: headers and rows are required');
  }

  // Build schema dynamically based on headers
  // For simplicity, we'll treat all columns as strings (UTF8)
  // In production, you might want to infer types (INT64, DOUBLE, BOOLEAN, etc.)
  const schemaFields = {};
  // Cache cleaned headers to avoid repeated processing
  const cleanedHeaders = [];
  const headerToCleanMap = new Map();
  
  headers.forEach((header, index) => {
    // Clean header name (Parquet doesn't like special characters)
    const cleanHeader = header.trim().replace(/[^a-zA-Z0-9_]/g, '_');
    // Ensure header is not empty after cleaning
    if (cleanHeader) {
      schemaFields[cleanHeader] = { type: 'UTF8' };
      cleanedHeaders.push(cleanHeader);
      headerToCleanMap.set(index, cleanHeader);
    } else {
      cleanedHeaders.push(null); // Placeholder for invalid headers
    }
  });

  if (Object.keys(schemaFields).length === 0) {
    throw new Error('No valid headers found after cleaning');
  }

  const schema = new parquet.ParquetSchema(schemaFields);

  // Create a temporary file path
  const tempFilePath = join(tmpdir(), `parquet-${randomUUID()}.parquet`);

  try {
    // Create a Parquet writer that writes to a file
    const writer = await parquet.ParquetWriter.openFile(schema, tempFilePath);

    // Write rows - optimized: pre-build row objects and batch process
    // Process in chunks to reduce memory pressure for very large datasets
    const BATCH_SIZE = 1000;
    for (let i = 0; i < rows.length; i += BATCH_SIZE) {
      const batch = rows.slice(i, Math.min(i + BATCH_SIZE, rows.length));
      
      // Process batch
      for (const row of batch) {
        const rowObject = {};
        // Use cached cleaned headers map for faster lookup
        headerToCleanMap.forEach((cleanHeader, originalIndex) => {
          rowObject[cleanHeader] = row[originalIndex] || '';
        });
        await writer.appendRow(rowObject);
      }
    }

    // Close writer (this will finalize the Parquet file)
    await writer.close();

    // Read the file into a buffer
    const buffer = await readFile(tempFilePath);

    // Clean up temporary file
    await unlink(tempFilePath).catch(err => {
      console.warn('Failed to delete temporary file:', tempFilePath, err);
    });

    return buffer;
  } catch (error) {
    // Clean up temporary file on error
    await unlink(tempFilePath).catch(() => {
      // Ignore errors during cleanup
    });
    throw error;
  }
}

// Save a new version of an existing project
router.post('/save-version', async (req, res) => {
  try {
    console.log('Save version request received');
    console.log('Session user:', req.session.user);
    
    // Check if user is authenticated
    if (!req.session.user || !req.session.user.sub) {
      console.log('Authentication check failed');
      return res.status(401).json({ error: 'Authentication required' });
    }

    const { dsGroupId, parentCommitId, csvData, modifications } = req.body;
    const cognitoSub = req.session.user.sub;

    console.log('Request data:', { dsGroupId, parentCommitId, hasCsvData: !!csvData, modificationsCount: modifications?.length || 0, cognitoSub });

    if (!dsGroupId || !parentCommitId || !csvData) {
      return res.status(400).json({ error: 'dsGroupId, parentCommitId, and CSV data are required' });
    }

    // Validate modifications array
    if (!modifications || !Array.isArray(modifications) || modifications.length === 0) {
      return res.status(400).json({ error: 'modifications array is required and must not be empty' });
    }

    // Verify user exists in database
    const userCheck = await query(
      'SELECT cognito_sub FROM users WHERE cognito_sub = $1',
      [cognitoSub]
    );

    if (userCheck.rows.length === 0) {
      console.log('User not found in database, inserting...');
      const userEmail = req.session.user.email || 'unknown@example.com';
      await query(
        'INSERT INTO users (cognito_sub, email) VALUES ($1, $2) ON CONFLICT (cognito_sub) DO NOTHING',
        [cognitoSub, userEmail]
      );
    }

    // Verify parent dataset exists and belongs to the user
    const parentCheck = await query(
      'SELECT ds_group_id, commit_id, cognito_sub FROM dataset WHERE ds_group_id = $1 AND commit_id = $2',
      [dsGroupId, parentCommitId]
    );

    if (parentCheck.rows.length === 0) {
      return res.status(404).json({ error: 'Parent dataset not found' });
    }

    if (parentCheck.rows[0].cognito_sub !== cognitoSub) {
      return res.status(403).json({ error: 'Unauthorized: Parent dataset does not belong to user' });
    }

    // Insert into dataset table with parent references
    // Reuse the same ds_group_id as the parent (same project)
    console.log('Inserting new version into dataset table...');
    const datasetResult = await query(
      `INSERT INTO dataset (ds_group_id, cognito_sub, parent_ds_group_id, parent_commit_id) 
       VALUES ($1, $2, $3, $4) 
       RETURNING ds_group_id, commit_id`,
      [dsGroupId, cognitoSub, dsGroupId, parentCommitId]
    );

    if (datasetResult.rows.length === 0) {
      console.error('Failed to create dataset entry - no rows returned');
      return res.status(500).json({ error: 'Failed to create dataset entry' });
    }

    const { ds_group_id, commit_id } = datasetResult.rows[0];
    console.log('New dataset version created:', { ds_group_id, commit_id });

    // Insert modifications into modifications table
    // Order should be 1, 2, 3, etc. for each modification
    console.log('Inserting modifications into modifications table...');
    if (modifications && modifications.length > 0) {
      for (let i = 0; i < modifications.length; i++) {
        const modification = modifications[i];
        const order = i + 1; // Order starts at 1
        const description = typeof modification === 'string' ? modification : modification.description || modification;
        
        // Truncate description to 255 characters to fit VARCHAR(255)
        const truncatedDescription = description.length > 255 ? description.substring(0, 252) + '...' : description;
        
        await query(
          `INSERT INTO modifications (commit_id, parent_commit_id, order_num, description) 
           VALUES ($1, $2, $3, $4)`,
          [commit_id, parentCommitId, order, truncatedDescription]
        );
      }
      console.log(`✓ Inserted ${modifications.length} modification(s) into modifications table`);
    }

    // Convert CSV data to Parquet
    console.log('Converting CSV to Parquet...');
    console.log('CSV data received:', {
      headersCount: csvData.headers?.length || 0,
      rowsCount: csvData.rows?.length || 0,
      firstRowSample: csvData.rows?.length > 0 ? csvData.rows[0]?.slice(0, 3) : 'no rows'
    });
    let parquetBuffer;
    try {
      parquetBuffer = await convertCsvToParquet(csvData);
      console.log(`✓ Parquet file created (${parquetBuffer.length} bytes)`);
    } catch (parquetError) {
      console.error('Error converting CSV to Parquet:', parquetError);
      return res.status(500).json({ error: 'Failed to convert CSV to Parquet', details: parquetError.message });
    }

    // Upload Parquet file to S3
    console.log('Attempting S3 upload...');
    console.log('  Parquet buffer size:', parquetBuffer.length, 'bytes');
    let s3Key = null;
    try {
      s3Key = await uploadParquetToS3(parquetBuffer, cognitoSub, ds_group_id, commit_id);
      console.log(`✓ File uploaded to S3: ${s3Key}`);
    } catch (s3Error) {
      console.error('❌ Error uploading to S3:', s3Error);
      return res.status(500).json({ error: 'Failed to upload to S3', details: s3Error.message });
    }

    res.json({ 
      success: true, 
      ds_group_id, 
      commit_id,
      s3Key,
      message: 'Project version saved successfully' 
    });
  } catch (error) {
    console.error('Error saving project version:', error);
    console.error('Error stack:', error.stack);
    res.status(500).json({ error: 'Failed to save project version', details: error.message });
  }
});

// Get all commits for a project (ds_group_id) with their relationships
router.get('/project/:dsGroupId/commits', async (req, res) => {
  try {
    if (!req.session.user || !req.session.user.sub) {
      return res.status(401).json({ error: 'Unauthorized' });
    }

    const { dsGroupId } = req.params;
    const cognitoSub = req.session.user.sub;

    console.log(`Fetching all commits for project ${dsGroupId}`);

    // Get all commits for this project
    const result = await query(
      `SELECT commit_id, parent_commit_id, parent_ds_group_id
       FROM dataset
       WHERE ds_group_id = $1 AND cognito_sub = $2
       ORDER BY commit_id`,
      [dsGroupId, cognitoSub]
    );

    if (result.rows.length === 0) {
      return res.status(404).json({ error: 'No commits found for this project' });
    }

    res.json({
      success: true,
      commits: result.rows
    });
  } catch (error) {
    console.error('Error fetching commits:', error);
    res.status(500).json({ error: 'Failed to fetch commits', details: error.message });
  }
});

// Get modifications for a specific commit (from parent to this commit)
router.get('/commit/:commitId/modifications', async (req, res) => {
  try {
    if (!req.session.user || !req.session.user.sub) {
      return res.status(401).json({ error: 'Unauthorized' });
    }

    const { commitId } = req.params;
    const cognitoSub = req.session.user.sub;

    console.log(`Fetching modifications for commit ${commitId}`);

    // Verify the commit belongs to the user
    const commitCheck = await query(
      'SELECT cognito_sub FROM dataset WHERE commit_id = $1',
      [commitId]
    );

    if (commitCheck.rows.length === 0) {
      return res.status(404).json({ error: 'Commit not found' });
    }

    if (commitCheck.rows[0].cognito_sub !== cognitoSub) {
      return res.status(403).json({ error: 'Forbidden: Commit does not belong to user' });
    }

    // Get modifications for this commit, ordered by order_num
    const result = await query(
      `SELECT order_num, description
       FROM modifications
       WHERE commit_id = $1
       ORDER BY order_num ASC`,
      [commitId]
    );

    res.json({
      success: true,
      modifications: result.rows
    });
  } catch (error) {
    console.error('Error fetching modifications:', error);
    res.status(500).json({ error: 'Failed to fetch modifications', details: error.message });
  }
});

// Get a specific commit's dataset
router.get('/project/:dsGroupId/commit/:commitId', async (req, res) => {
  try {
    if (!req.session.user || !req.session.user.sub) {
      return res.status(401).json({ error: 'Unauthorized' });
    }

    const { dsGroupId, commitId } = req.params;
    const cognitoSub = req.session.user.sub;

    console.log(`Fetching commit ${commitId} for project ${dsGroupId}`);

    // Verify the commit belongs to the user and project
    const commitCheck = await query(
      'SELECT commit_id, cognito_sub FROM dataset WHERE ds_group_id = $1 AND commit_id = $2',
      [dsGroupId, commitId]
    );

    if (commitCheck.rows.length === 0) {
      return res.status(404).json({ error: 'Commit not found' });
    }

    if (commitCheck.rows[0].cognito_sub !== cognitoSub) {
      return res.status(403).json({ error: 'Forbidden: Commit does not belong to user' });
    }

    // Download Parquet file from S3
    let csvData;
    try {
      const parquetBuffer = await downloadParquetFromS3(cognitoSub, dsGroupId, commitId);
      csvData = await convertParquetToCsv(parquetBuffer);
      console.log(`✓ Converted Parquet to CSV: ${csvData.headers.length} columns, ${csvData.rows.length} rows`);
    } catch (s3Error) {
      if (s3Error.name === 'NoSuchKey') {
        return res.status(404).json({ 
          error: 'File not found in S3',
          message: 'The Parquet file for this commit does not exist in S3. The project may need to be re-uploaded.'
        });
      }
      throw s3Error;
    }

    res.json({
      success: true,
      csvData
    });
  } catch (error) {
    console.error('Error fetching commit:', error);
    res.status(500).json({ error: 'Failed to fetch commit', details: error.message });
  }
});

export default router;

